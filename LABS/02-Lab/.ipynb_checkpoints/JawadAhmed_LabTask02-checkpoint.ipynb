{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 02 NLTK: Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Bigrams & Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "words = sorted(set(text1))[280:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19037\n"
     ]
    }
   ],
   "source": [
    "print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "longwords = [w for w in words if len(w) > 16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cannibalistically',\n",
       " 'characteristically',\n",
       " 'circumnavigations',\n",
       " 'comprehensiveness',\n",
       " 'indispensableness',\n",
       " 'preternaturalness',\n",
       " 'subterraneousness',\n",
       " 'superstitiousness',\n",
       " 'uncomfortableness',\n",
       " 'uncompromisedness',\n",
       " 'uninterpenetratingly']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "longwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fdist1 = FreqDist(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "high_freq = [w for w in words if fdist1[w] > 500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ahab', 'But', 'I', 'The', 'a', 'all', 'an', 'and', 'are', 'as', 'at', 'be', 'but', 'by', 'for', 'from', 'had', 'have', 'he', 'him', 'his', 'in', 'into', 'is', 'it', 'like', 'man', 'me', 'more', 'my', 'not', 'now', 'of', 'on', 'one', 'or', 'out', 's', 'ship', 'so', 'some', 'that', 'the', 'their', 'then', 'there', 'they', 'this', 'to', 'up', 'upon', 'was', 'were', 'whale', 'when', 'which', 'with', 'you']\n"
     ]
    }
   ],
   "source": [
    "print(high_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import latexify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math \n",
    "\n",
    "@latexify.function\n",
    "def IDF(corpus_list, token):\n",
    "    document_frequency = sum(1 for corpus in corpus_list if token in corpus)\n",
    "\n",
    "    # Add 1 to both numerator and denominator for smoothing\n",
    "    inverse_document_frequency = math.log((len(corpus_list) + 1) / (document_frequency + 1), 10)\n",
    "    return inverse_document_frequency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$ \\displaystyle \\begin{array}{l} \\mathrm{document\\_frequency} = \\sum_{\\mathopen{}\\left( \\mathrm{corpus} \\in \\mathrm{corpus\\_list} \\mathclose{}\\right) \\land \\mathopen{}\\left( \\mathrm{token} \\in \\mathrm{corpus} \\mathclose{}\\right)}^{} \\mathopen{}\\left({1}\\mathclose{}\\right) \\\\ \\mathrm{inverse\\_document\\_frequency} = \\log \\mathopen{}\\left( \\frac{\\mathrm{len} \\mathopen{}\\left( \\mathrm{corpus\\_list} \\mathclose{}\\right) + 1}{\\mathrm{document\\_frequency} + 1}, 10 \\mathclose{}\\right) \\\\ \\mathrm{IDF}(\\mathrm{corpus\\_list}, \\mathrm{token}) = \\mathrm{inverse\\_document\\_frequency} \\end{array} $$"
      ],
      "text/plain": [
       "<latexify.ipython_wrappers.LatexifiedFunction at 0x7fd2f84a6bd0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "high_freq_idf = [w for w in words if IDF(text1, w) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19027"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(high_freq_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ABOUT', 'ACCOUNT', 'ADDITIONAL', 'ADVANCING', 'ADVENTURES', 'AFFGHANISTAN', 'AFRICA', 'AFTER', 'AGAINST', 'AHAB']\n"
     ]
    }
   ],
   "source": [
    "print(high_freq_idf[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eign_words = [w for w in words if w.endswith('eign') ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sovereign', 'foreign', 'reign', 'sovereign']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eign_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sovereign\n",
      "foreign\n",
      "reign\n",
      "sovereign\n"
     ]
    }
   ],
   "source": [
    "for w in words:\n",
    "    if w.endswith('eign'):\n",
    "        print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "260818"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(bigrams(text1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Sperm', 'Whale'),\n",
       " ('Moby', 'Dick'),\n",
       " ('White', 'Whale'),\n",
       " ('old', 'man'),\n",
       " ('Captain', 'Ahab'),\n",
       " ('sperm', 'whale'),\n",
       " ('Right', 'Whale'),\n",
       " ('Captain', 'Peleg'),\n",
       " ('New', 'Bedford'),\n",
       " ('Cape', 'Horn'),\n",
       " ('cried', 'Ahab'),\n",
       " ('years', 'ago'),\n",
       " ('lower', 'jaw'),\n",
       " ('never', 'mind'),\n",
       " ('Father', 'Mapple'),\n",
       " ('cried', 'Stubb'),\n",
       " ('chief', 'mate'),\n",
       " ('white', 'whale'),\n",
       " ('ivory', 'leg'),\n",
       " ('one', 'hand')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1.collocation_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('[', 'Moby', 'Dick'),\n",
       " ('Moby', 'Dick', 'by'),\n",
       " ('Dick', 'by', 'Herman'),\n",
       " ('by', 'Herman', 'Melville'),\n",
       " ('Herman', 'Melville', '1851'),\n",
       " ('Melville', '1851', ']'),\n",
       " ('1851', ']', 'ETYMOLOGY'),\n",
       " (']', 'ETYMOLOGY', '.'),\n",
       " ('ETYMOLOGY', '.', '('),\n",
       " ('.', '(', 'Supplied')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "\n",
    "list(ngrams(text1, 3))[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('AFTER', 'EXCHANGING', 'HAILS'),\n",
       " ('Anacharsis', 'Clootz', 'deputation'),\n",
       " ('CAULKING', 'ITS', 'SEAMS'),\n",
       " ('ELIZABETH', 'OAKES', 'SMITH'),\n",
       " ('Et', 'tu', 'Brute'),\n",
       " ('Ex', 'officio', 'professors'),\n",
       " ('Fogo', 'Von', 'Slack'),\n",
       " ('Ganders', 'formally', 'indite'),\n",
       " ('Kentucky', 'Mammoth', 'Cave'),\n",
       " ('LANTERNS', 'BUSILY', 'FILING')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.collocations import TrigramCollocationFinder, TrigramAssocMeasures\n",
    "\n",
    "TrigramCollocationFinder.from_words(text1).nbest(TrigramAssocMeasures().pmi, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10 frequently occuring Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Frequently Occurring Bigrams in Text1:\n",
      "[((',', 'and'), 2607), (('of', 'the'), 1847), ((\"'\", 's'), 1737), (('in', 'the'), 1120), ((',', 'the'), 908), ((';', 'and'), 853), (('to', 'the'), 712), (('.', 'But'), 596), ((',', 'that'), 584), (('.', '\"'), 557)]\n",
      "\n",
      "Top 10 Frequently Occurring Bigrams in Text2:\n",
      "[((',', 'and'), 1598), ((\"'\", 's'), 700), ((';', 'and'), 605), (('Mrs', '.'), 529), (('of', 'the'), 430), (('to', 'be'), 428), (('.\"', '\"'), 428), ((',', '\"'), 392), (('.', '\"'), 369), (('in', 'the'), 348)]\n",
      "\n",
      "Top 10 Frequently Occurring Bigrams in Text3:\n",
      "[((',', 'and'), 1491), (('.', 'And'), 1038), (('of', 'the'), 372), (('in', 'the'), 287), ((';', 'and'), 262), (('said', ','), 259), ((\"'\", 's'), 255), (('And', 'he'), 192), (('And', 'the'), 185), (('said', 'unto'), 178)]\n"
     ]
    }
   ],
   "source": [
    "from nltk import bigrams, FreqDist\n",
    "\n",
    "def top_frequent_bigrams(text, n = 10):\n",
    "    bigrams_list = list(bigrams(text))\n",
    "    bigram_freq = FreqDist(bigrams_list)\n",
    "    \n",
    "    return bigram_freq.most_common(n)\n",
    "\n",
    "text1_bigrams = top_frequent_bigrams(text1)\n",
    "text2_bigrams = top_frequent_bigrams(text2)\n",
    "text3_bigrams = top_frequent_bigrams(text3)\n",
    "\n",
    "print(\"Top 10 Frequently Occurring Bigrams in Text1:\")\n",
    "print(text1_bigrams)\n",
    "print(\"\\nTop 10 Frequently Occurring Bigrams in Text2:\")\n",
    "print(text2_bigrams)\n",
    "print(\"\\nTop 10 Frequently Occurring Bigrams in Text3:\")\n",
    "print(text3_bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 frequently occuring Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 Frequently Occurring Trigrams in Text1:\n",
      "[((',', 'and', 'the'), 187), (('don', \"'\", 't'), 103), (('of', 'the', 'whale'), 101), ((',', 'in', 'the'), 93), ((',', 'then', ','), 87)]\n",
      "\n",
      "Top 5 Frequently Occurring Trigrams in Text2:\n",
      "[(('Mrs', '.', 'Jennings'), 230), (('Mrs', '.', 'Dashwood'), 121), ((',', 'however', ','), 88), ((',', 'and', 'the'), 87), (('.', 'Mrs', '.'), 80)]\n",
      "\n",
      "Top 5 Frequently Occurring Trigrams in Text3:\n",
      "[(('.', 'And', 'he'), 162), (('.', 'And', 'the'), 158), (('the', 'land', 'of'), 101), (('he', 'said', ','), 86), (('And', 'he', 'said'), 84)]\n"
     ]
    }
   ],
   "source": [
    "from nltk import trigrams, FreqDist\n",
    "from nltk.book import text1, text2, text3\n",
    "\n",
    "def top_frequent_trigrams(text, n=5):\n",
    "    trigrams_list = list(trigrams(text))\n",
    "    trigram_freq = FreqDist(trigrams_list)\n",
    "    return trigram_freq.most_common(n)\n",
    "\n",
    "text1_trigrams = top_frequent_trigrams(text1, n=5)\n",
    "text2_trigrams = top_frequent_trigrams(text2, n=5)\n",
    "text3_trigrams = top_frequent_trigrams(text3, n=5)\n",
    "\n",
    "print(\"\\nTop 5 Frequently Occurring Trigrams in Text1:\")\n",
    "print(text1_trigrams)\n",
    "print(\"\\nTop 5 Frequently Occurring Trigrams in Text2:\")\n",
    "print(text2_trigrams)\n",
    "print(\"\\nTop 5 Frequently Occurring Trigrams in Text3:\")\n",
    "print(text3_trigrams)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of Words with Length > 16:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of Words with Length > 16 in Text1: 14\n",
      "Number of Words with Length > 16 in Text2: 3\n",
      "Number of Words with Length > 16 in Text3: 0\n"
     ]
    }
   ],
   "source": [
    "def count_words_length_greater_than(text, length=16):\n",
    "    long_words = [word for word in text if len(word) > length]\n",
    "    return len(long_words)\n",
    "\n",
    "text1_long_words_count = count_words_length_greater_than(text1)\n",
    "text2_long_words_count = count_words_length_greater_than(text2)\n",
    "text3_long_words_count = count_words_length_greater_than(text3)\n",
    "\n",
    "print(\"\\nNumber of Words with Length > 16 in Text1:\", text1_long_words_count)\n",
    "print(\"Number of Words with Length > 16 in Text2:\", text2_long_words_count)\n",
    "print(\"Number of Words with Length > 16 in Text3:\", text3_long_words_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of words with frequency > 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of Words with Frequency > 500 in Text1: 67\n",
      "Number of Words with Frequency > 500 in Text2: 45\n",
      "Number of Words with Frequency > 500 in Text3: 13\n"
     ]
    }
   ],
   "source": [
    "def count_words_frequency_greater_than(text, frequency=500):\n",
    "    word_freq = FreqDist(text)\n",
    "    high_frequency_words = [word for word, freq in word_freq.items() if freq > frequency]\n",
    "    return len(high_frequency_words)\n",
    "\n",
    "text1_high_freq_words_count = count_words_frequency_greater_than(text1)\n",
    "text2_high_freq_words_count = count_words_frequency_greater_than(text2)\n",
    "text3_high_freq_words_count = count_words_frequency_greater_than(text3)\n",
    "\n",
    "print(\"\\nNumber of Words with Frequency > 500 in Text1:\", text1_high_freq_words_count)\n",
    "print(\"Number of Words with Frequency > 500 in Text2:\", text2_high_freq_words_count)\n",
    "print(\"Number of Words with Frequency > 500 in Text3:\", text3_high_freq_words_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of ending in “ed”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of Words Ending in 'ed' in Text1: 8491\n",
      "Number of Words Ending in 'ed' in Text2: 4866\n",
      "Number of Words Ending in 'ed' in Text3: 1238\n"
     ]
    }
   ],
   "source": [
    "def count_words_ending_in_ed(text):\n",
    "    ed_words = [word for word in text if word.endswith(\"ed\")]\n",
    "    return len(ed_words)\n",
    "\n",
    "text1_ed_words_count = count_words_ending_in_ed(text1)\n",
    "text2_ed_words_count = count_words_ending_in_ed(text2)\n",
    "text3_ed_words_count = count_words_ending_in_ed(text3)\n",
    "\n",
    "print(\"\\nNumber of Words Ending in 'ed' in Text1:\", text1_ed_words_count)\n",
    "print(\"Number of Words Ending in 'ed' in Text2:\", text2_ed_words_count)\n",
    "print(\"Number of Words Ending in 'ed' in Text3:\", text3_ed_words_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Accessing Corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gutenberg_sc = nltk.corpus.gutenberg.words('shakespeare-caesar.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25833"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gutenberg_sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "brown.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nltk.corpus.reader.tagged.CategorizedTaggedCorpusReader"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(brown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package abc to /home/jadi/nltk_data...\n",
      "[nltk_data]   Package abc is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Download the necessary data\n",
    "nltk.download('abc')\n",
    "\n",
    "# Load the file from the NLTK corpus\n",
    "psh_txt = nltk.corpus.abc.raw('science.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "words = nltk.word_tokenize(psh_txt)\n",
    "psh_bigrams = list(ngrams(words, 2))\n",
    "psh_trigrams = list(ngrams(words, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /home/jadi/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'Fulton'), ('Fulton', 'County'), ('County', 'Grand'), ('Grand', 'Jury'), ('Jury', 'said'), ('said', 'Friday'), ('Friday', 'an'), ('an', 'investigation'), ('investigation', 'of'), ('of', \"Atlanta's\")]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "from nltk import ngrams\n",
    "\n",
    "# Download the Brown corpus if not already downloaded\n",
    "nltk.download('brown')\n",
    "\n",
    "# Get the wordlist from the Brown corpus and create bigrams\n",
    "wordlist = brown.words()\n",
    "bigramlist = list(ngrams(wordlist, 2))\n",
    "\n",
    "# Print the first 10 bigrams as an example\n",
    "print(bigramlist[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Generating Random Text with Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cfd = nltk.ConditionalFreqDist(bigramlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ConditionalFreqDist with 56057 conditions>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_model(cfdist, word, num):\n",
    "    for i in range(num):\n",
    "        print(word, end=' ')\n",
    "        word = cfdist[word].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from tabulate import tabulate\n",
    "# # Print a limited number of conditions and their frequency distributions\n",
    "# print(\"Conditions:\", cfd.conditions()[:5])  # Print the first 5 conditions\n",
    "# for condition in cfd.conditions()[:5]:\n",
    "#     freq_dist = FreqDist(cfd[condition])\n",
    "#     print(f\"Condition: {condition}\")\n",
    "#     print(tabulate(freq_dist.items(), headers=[\"Event\", \"Frequency\"]))\n",
    "#     print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generate_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m generate_model(cfd, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mToday\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m10\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'generate_model' is not defined"
     ]
    }
   ],
   "source": [
    "generate_model(cfd, 'Today', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today , and the same time , and the same time , and the same time , and the same time , and the same time , and the same "
     ]
    }
   ],
   "source": [
    "generate_model(cfd, 'Today', 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 30 word generated sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56057"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cfd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 word generated sentence\n"
     ]
    }
   ],
   "source": [
    "print('30 word generated sentence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today , and the same time , and the same time , and the same time , and the same time , and the same time , and the same "
     ]
    }
   ],
   "source": [
    "generate_model(cfd, 'Today', 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
